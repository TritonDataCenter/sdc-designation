---
title: Designation API (DAPI)
apisections: Allocation, Allocation Algorithms, Changelog
markdown2extras: wiki-tables, code-friendly
---

# DAPI

* Repository: git@git.joyent.com:dapi.git
* Browsing: <https://mo.joyent.com/dapi>
* Docs: <https://mo.joyent.com/docs/dapi>
* Who: Andres Rodriguez, Marsell Kukuljevic
* Tickets/bugs: <https://devhub.joyent.com/jira/browse/DAPI>



# Introduction to Designation API

## What is DAPI?

The Designation API's purpose is to respond with a compute node on which to
place a new machine or machine reservation. It needs to be passed a full payload
of the machine in question, and a list of all compute nodes extracted from CNAPI
to be considered for allocation.



# Getting Started

DAPI can run without any other SDC dependency. Since its purpose is to select a
compute node to allocate a VM to, DAPI relies solely on the request parameters
in order to do its job. With this in mind, a DAPI instance can run on any
SmartOS machine for testing purposes without a full SDC install.

    # Get the source and build.
    git clone git@git.joyent.com:dapi.git
    cd dapi/
    make all

    # Setup config.
    cp config.coal.json config.json
    # Edit config as needed
    vi config.json

    # node bin/server.js



# Allocation

Currently, there are three parameters required for allocating a VM to a
server: VM payload, image manifest, and the servers list. Optionally, a package
payload may be provided as well.



## VM Payload

The VM payload describes the VM to be allocated. For example, how much disk it
needs, how much RAM, and so forth. DAPI accepts arbitrary descriptions of the
VM, but the current version only considers the following characteristics:

|| **Attribute** || **Type** || **Description**                                      ||
|| ram           || integer  || RAM required, in MiB                                 ||
|| quota         || integer  || Disk required, in MiB                                ||
|| cpu_cap       || integer  || Max CPU allowed, in percent                          ||
|| traits        || hash     || Traits that a compute node must match                ||
|| nic_tags      || array    || If VLAN filtering is enabled, nic_tags needed for VM ||
|| owner_uuid    || string   || UUID of the new zone's owner                         ||

The following are a couple of valid VM payload examples. As long as the
***ram*** attribute is present, the object will pass validation. 

    # Simplest example

    {
      "ram": 2048
    }

    # More complete VM payload, similar to `vmadm get <uuid>`

    {
      "uuid": "ef375f03-57ca-44a9-bc8d-63aec09fbc37",
      "brand": "joyent",
      "alias": "assets1",
      "ram": 64,
      "max_swap": 256,
      "quota": 10240,
      "cpu_cap": 100,
      "cpu_shares": 1,
      "max_lwps": 1000,
      "zpool": "zones",
      "zfs_io_priority": 10,
      "owner_uuid": "930896af-bf8c-48d4-885c-6573a94b1853",
      "nics": [],
      "customer_metadata": {},
      "internal_metadata": {},
      "tags": {},
      "traits": { "ssd": true }
    }



## Image Manifest

The image manifest is what IMGAPI provides. It contains various information and
restrictions on VMs and CNs required by the dataset which will be used to create
a VM.

In contrast to VMs, image manifests are more strictly validated. Currently DAPI
understands the following characteristics:

|| **Attribute** || **Type** || **Description**                          ||
|| traits        || hash     || Traits that a compute node must match    ||
|| requirements  || hash     || Various restrictions that a CN must meet ||
|| image_size    || integer  || Size in MiB of KVM root filesystem       ||

Currently, DAPI considers the following attributes in ***requirements***, all
attributes being optional:

|| **Attribute** || **Type** || **Description**                             ||
|| min_ram       || integer  || Minimum RAM in MiB needed for a zone        ||
|| max_ram       || integer  || Maximum RAM in MiB allowed for a zone       ||
|| min_platform  || hash     || Minimum platform on a CN for an SDC version ||
|| max_platform  || hash     || Minimum platform on a CN for an SDC version ||

min_platform and max_platform are hashes with SDC versions as keys, and CN
platform versions as values. This is used to ensure that datasets which require
certain features from an underlying platform do not end up used in a VM on a CN
running a platform that doesn't support those features (the platform either
being too old, or no longer supporting a removed feature).

An example of a valid manifest:

    {
      "image_size": 10240,
      "traits": {
        "ssd": true
      },
      "requirements": {
        "min_ram": 1024,
        "min_platform": {
          "6.5": "20121211T203034Z",
          "7.0": "20121117T221253Z"
        }
      }
    }



## Servers List

The same way the VM payload follows the format that vmadm provides, the servers
payload that is required by the allocation endpoint is expected to be CNAPI
"compatible". CNAPI is the entity that manages and exposes all the information
related to servers in an SDC install, and therefore it provides the standard
representation for a server object.

In contrast to VMs, server payloads are more strictly validated. DAPI accepts
arbitrary descriptions of each server, but the current version only considers
the following characteristics:

|| **Attribute**          || **Type** || **Description**                                 ||
|| uuid                   || string   || UUID of server                                  ||
|| memory_available_bytes || integer  || RAM available for allocation, in bytes          ||
|| memory_total_bytes     || integer  || Total RAM on server, in bytes                   ||
|| reservation_ratio      || float    || Amount of memory to reserve for ZFS ARC and OS  ||
|| reserved               || boolean  || If the server has been reserved by an operator  ||
|| setup                  || boolean  || If the server has been setup                    ||
|| 'Zpool Size in GiB'    || integer  || Total disk space on server, in GiB              ||
|| vms                    || hash     || Description of VMs already on server            ||
|| traits                 || hash     || Traits on a server                              ||

The ***vms*** attribute is a hash. Each key is a VM UUID, and each value is a
hash with the following format:

|| **Attribute**       || **Type** || **Description**                         ||
|| cpu_cap             || integer  || Max CPU allowed, in percent             ||
|| max_physical_memory || integer  || RAM promised to this VM, in MiB         ||
|| quota               || integer  || Disk promised to this VM, in MiB        ||
|| owner_uuid          || string   || UUID of VM's owner                      ||

As an example of a valid servers payload:

    [
      {
        "uuid": "cdc1b052-1df9-4a3d-b34d-3f0afadac883",
        "memory_total_bytes": 2147483648,
        "memory_available_bytes": 1073741824,
        "reservation_ratio": 0.15,
        "reserved": false,
        "setup": true,
        "Zpool Size in GiB": 1024,
        "vms": {
          "fbbbdc6d-011c-4931-9420-c702f53e4154": {
            "max_physical_memory": 1024,
            "quota": 10240,
            "cpu_cap": 350,
            "owner_uuid": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
          }
        }
      }, {
        "uuid": "82c53a64-b19c-4266-bf16-3ee1d34318d2",
        "memory_total_bytes": 1073741824,
        "memory_available_bytes": 536870912
        "reservation_ratio": 0.25,
        "reserved": false,
        "setup": false,
        "Zpool Size in GiB": null,
        "vms": {}
      }
    ]


## Package Payload

The package payload is optional, and can be used to fill in details that the VM
payload does not provide, like certain defaults. Right now packages are used by
DAPI for two things: traits and overprovision ratios.

|| **Attribute**         || **Type** || **Description**                                  ||
|| traits                || hash     || Traits that a compute node must match            ||
|| min_platform          || hash     || Minimum platform on a CN for an SDC version      ||
|| overprovision_cpu     || float    || Ratio for maximum overprovisioning on CPU        ||
|| overprovision_memory  || float    || Ratio for maximum overprovisioning on RAM        ||
|| overprovision_storage || float    || Ratio for maximum overprovisioning on disk space ||
|| overprovision_io      || float    || Currently ignored                                ||
|| overprovision_network || float    || Currently ignored                                ||

Traits that the package provides can be overridden by traits in the VM payload.

min_platform follows the same rules as min_platform in an image. Please see the
Image Manifest section above.

Each overprovision ratio is optional. If a ratio is provided, DAPI will attempt
to ensure that the ratio (or better) is kept. If a ratio is not provided, DAPI
will assume that overprovisioning on that resource does not matter, and will
completely disregard maintaining any ratio.

E.g. if you want to guarantee that CPU is not overprovisioned, provide an
overprovision_cpu of 1.0. If you'd like to overprovision CPU by two, use
overprovision_cpu of 2.0. If you really don't care whether it's overprovisioned
or not, do not provide an overprovision_cpu.

By default, if a package does not provide any overprovision ratios at all,
DAPI will default to using a single ratio: an overprivison_memory of 1.0.

An example of a valid package payload:

    {
      "overprovision_memory": 1.5,
      "overprovision_storage": 1.0,
      "traits": {
        "ssd": true
      }
    }



## Allocate Endpoint (POST /allocation)

Unique endpoint for allocating a VM to a server.

Both of the following inputs are required:

|| **Param** || **Type** || **Description**                              ||
|| vm        || hash     || VM Payload, see "VM Payload" above           ||
|| servers   || hash     || List of servers, see "Servers Payload" above ||

The endpoint will either return a 409, or a 200:

|| **HTTP code** || **Description**                                                 ||
|| 409           || No compute node found to satisfy the request                    ||
|| 200           || Compute node found, description of chosen compute node returned ||

The description returned in the case of a HTTP 200 is the same JSON
representation of the compute node which was provided to DAPI in the servers
payload.



### Programmatic Example

    var servers = [ {
      uuid: 'cdc1b052-1df9-4a3d-b34d-3f0afadac883',
      memory_total_bytes: 2147483648,
      memory_available_bytes: 1073741824,
      reservation_ratio: 0.15,
      reserved: false,
      setup: true,
      'Zpool Size in GiB': 1024,
      vms: {
        'fbbbdc6d-011c-4931-9420-c702f53e4154': {
          max_physical_memory: 1024,
          quota: 10240,
          owner_uuid: '9b81f9e7-55e1-4e00-a8f7-917bd054b320'
        }
      }
    }, {
      uuid: '82c53a64-b19c-4266-bf16-3ee1d34318d2',
      memory_total_bytes: 1073741824,
      memory_available_bytes: 536870912
      reservation_ratio: 0.25,
      reserved: false,
      setup: false,
      'Zpool Size in GiB': null,
      vms: {}
    } ];

    var data = { servers: servers, vm: { ram: 2048 } };

    restify.post('/allocation', data, function (err, req, res, server) {
      assert.ok(server);
    });



# Allocation Algorithms

DAPI provides the ability for users to install custom allocation algorithms and
test several allocation strategies without the need to modify the application
source code. An allocation algorithm takes a list of servers and various other
pieces of data as input, and returns a filtered list of servers.


By default -- if DAPI's configuration file doesn't specify an
`allocationDescription` -- the following chain of algorithms will be used (see
below for description of what each one does):

    ['pipe', 'hard-filter-setup',
             'calculate-server-unreserved',
             'hard-filter-overprovision-ratios',
             'hard-filter-min-ram',
             'hard-filter-min-disk',
             'hard-filter-min-cpu',
             'hard-filter-running',
             'hard-filter-reserved',
             'hard-filter-headnode',
             'hard-filter-platform-versions',
             'hard-filter-traits',
             'soft-filter-recent-servers'
             ['or', 'hard-filter-large-servers',
                    'identity'],
             ['or', 'hard-filter-owner-same-racks',
                    'hard-filter-owner-same-servers',
                    'soft-filter-owner-many-zones'],
             'sort-min-ram',
             'pick-weighted-random']

Also see DEFAULT_DESC in ***$DAPI_ROOT/lib/allocator.js***.

This default description gives reasonable allocation results for a datacenter;
make sure to do your research before modifying it. The description is organized
as a list of algorithms and lists, each list prefixed with a command. For
example:

    ['pipe', 'hard-filter-min-ram',
             'hard-filter-running',
             ['or', 'hard-filter-large-servers',
                    'hard-filter-recent-servers'],
             'pick-weighted-random']

The 'pipe' and 'or' are commands, and the rest of the elements in the list will
be executed in that context. 'pipe' forms a pipeline, feeding the output (list
of servers) from one algorithms to the input of the next. 'or' feeds an
identical list of servers to each plugin in turn, until one of the plugins
returns a non-empty list of servers.

To change to default chain, edit DAPI's configuration file
(***$DAPI_ROOT/config.json***). Edit or add the 'algorithms' attribute:

    ...
    "allocationDescription": ["pipe", "hard-filter-setup",
                                      "hard-filter-min-ram",
                                      "pick-random"],
    ...

In this case, a pipeline of only those three algorithms would be used in
selecting a compute node for the VM.

Custom algorithms can be added under ***$DAPI_ROOT/lib/algorithms/***, but be
aware that you'll need to add the custom file back after any DAPI zone upgrade.



## Provided Algorithms

|| **Name**                        || **Action**                                                  ||
|| calculate-server-unreserved     || Does some free-resource calculations that are used by other plugins  ||
|| hard-filter-headnode            || Removes any headnodes                                       ||
|| hard-filter-large-servers       || Removes the top 15% servers with the most available RAM     ||
|| hard-filter-min-cpu             || Removes CNs with insufficient CPU                           ||
|| hard-filter-min-disk            || Removes CNs with insufficient disk                          ||
|| hard-filter-min-ram             || Removes CNs with insufficient RAM                           ||
|| hard-filter-overprovision-ratios || Removes CNs with different overprovision ratios than the request    ||
|| hard-filter-owner-same-racks    || Removes racks already containing an owner's VM(s)           ||
|| hard-filter-owner-same-servers  || Removes servers already containing an owner's VM(s)         ||
|| hard-filter-platform-versions   || Removes servers that don't pass image manifest platform requirements ||
|| hard-filter-reserved            || Removes reserved CNs                                        ||
|| hard-filter-running             || Removes CNs which are not running                           ||
|| hard-filter-setup               || Removes CNs which are not setup                             ||
|| hard-filter-traits              || Removes CNs with traits that cannot fulfill VM traits       ||
|| hard-filter-vlans               || Removes CNs which do not have required nic tags             ||
|| identity                        || Returns the same servers it received                        ||
|| override-overprovisioning       || Substitutes package and server overprovision data for own defaults   ||
|| pick-random                     || Pick a random CN from a list                                ||
|| pick-weighted-random            || Pick one of top 20% of CNs so far                           ||
|| soft-filter-large-servers       || Tries to reserve some servers for large VMs                 ||
|| soft-filter-recent-servers      || Tries to ignore recently allocated-to CNs, to prevent races ||
|| sort-2adic                      || Order CNs by 2adic ordering of available RAM                ||
|| sort-min-ram                    || Order CNs by how little available RAM is left               ||
|| sort-ram                        || Order CNs by available RAM                                  ||

The allocation pipeline typically starts with the hard filters, then soft
filters, then sorters, and finally a picker.

Hard filters remove CNs from allocation consideration because the compute node
fails to fulfill some requirement. Soft filters remove some compute nodes from
consideration if there's still enough compute nodes left afterwards to
effectively allocate with. Sorters order compute nodes by how desirable they are
to fulfill this allocation request -- most desirable come first. Lastly, the
pickers pick one of the compute nodes; some pickers take order into account, and
some do not.



## Traits

Although traits are only used by one algorithm, they serve the important purpose
of describing arbitrary user-defined requirements for a VM and image manifest,
and matching those requirements with CNs that can fulfill them. Ergo I'll
describe how they work here.

Imagine for a moment you'd like to reserve some servers for just one customer.
In this cause you'd add a customer trait to both the CNs and to the VM request.
E.g.

Server object:

    {
      "uuid": "cdc1b052-1df9-4a3d-b34d-3f0afadac883",
      "memory_total_bytes": 2147483648,
      ...
      "traits": {
        "customer": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
      }
    }

VM request:

    {
      "ram": 2048,
      "traits": {
        "customer": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
      }
    }

It does not need to be named ***customer*** specifically; it could be named
***owner*** or something else, so long as the attribute name matches between
both the server trait and the VM request trait. The VM request requires a
compute node with a trait named "customer" and value
"9b81f9e7-55e1-4e00-a8f7-917bd054b320". The compute node has both that name and
value in the traits, so it can fulfill this request.

The same applies for booleans:

    {
      "ram": 2048,
      "traits": {
        "ssd": true
      }
    }

Any compute node that has a single trait named ***ssd*** and a value of true
will match.

Multiple traits can be used:

    {
      "ram": 2048,
      "traits": {
        "ssd": false,
        "manta": true,
        "customer": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
      }
    }

In which case only compute nodes that have traits like this will match:

    {
      "uuid": "cdc1b052-1df9-4a3d-b34d-3f0afadac883",
      "memory_total_bytes": 2147483648,
      ...
      "traits": {
        "ssd": false,
        "manta": true,
        "customer": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
      }
    }

Things get a bit more complicated with arrays: traits also allow arrays of
strings for a given name, in both VM requests and on compute nodes. If a VM
trait has a string and a compute node's associated trait has an array of
strings, then that compute node will match if the VM's string matches any of the
ones in the compute node's. The opposite applies as well, with the compute node
trait having an array of strings, and the VM trait being a single string. An
example:

    {
      "ram": 2048,
      "traits": {
        "customer": "9b81f9e7-55e1-4e00-a8f7-917bd054b320"
      }
    }

    {
      "uuid": "cdc1b052-1df9-4a3d-b34d-3f0afadac883",
      "memory_total_bytes": 2147483648,
      ...
      "traits": {
        "customer": ["9b81f9e7-55e1-4e00-a8f7-917bd054b320",
                     "82c53a64-b19c-4266-bf16-3ee1d34318d2"]
      }
    }

The above compute node will match because the VM request is looking for customer
9b81f9e7..., and the compute node can fulfill 9b81f9e7... and 82c53a64... .

Lastly, if a VM trait contains an array of strings, and the compute node does
as well, the compute node will match if there's an intersection in the array
values. In other words, if any values in the VM trait matches any values in the
compute node's trait. E.g. the following will match because "richmond-a" is in
common between the two (and ssd is true for both too, just to demonstrate):

    {
      "ram": 2048,
      "traits": {
        "hw": ["richmond-a", "mantis-shrimp"],
        "ssd": true
      }
    }

    {
      "uuid": "cdc1b052-1df9-4a3d-b34d-3f0afadac883",
      "memory_total_bytes": 2147483648,
      ...
      "traits": {
        "hw": ["richmond-b", "richmond-a"]
        "ssd": true
      }
    }

Image manifests also can contain traits, and they take priority over traits in
a VM payload. A union is made between traits between the VM payload and image
manifest, attributes from the image manifest's traits taking priority when both
the VM payload and image manifest share some attributes in their traits, and
the resulting traits union is used to match against CN traits.



# Pinging

For consistency with other SDC services, DAPI responds to a GET /ping. If it
responds with a 200, DAPI is online.
